{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 逻辑回归 Python实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### prepare the dataset\n",
    "iris = load_iris()\n",
    "df = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
    "df['label'] = iris.target\n",
    "df.columns = ['sepal_l','sepal_w','petal_l','petal_w','label']\n",
    "df = df[df['label'].apply(lambda x:x in [0,1])]\n",
    "X = np.array(df.iloc[:,:4])\n",
    "Y = np.array(df['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal_l</th>\n",
       "      <th>sepal_w</th>\n",
       "      <th>petal_l</th>\n",
       "      <th>petal_w</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal_l  sepal_w  petal_l  petal_w  label\n",
       "0      5.1      3.5      1.4      0.2      0\n",
       "1      4.9      3.0      1.4      0.2      0\n",
       "2      4.7      3.2      1.3      0.2      0\n",
       "3      4.6      3.1      1.5      0.2      0\n",
       "4      5.0      3.6      1.4      0.2      0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LR():\n",
    "    def __init__(self, method = 'gd', max_iter = 5000, tol = 0.00001, alpha = 0.1):\n",
    "        \"\"\"\n",
    "        'gd' gradient descent\n",
    "        'sgd' stochastic gradient descent\n",
    "        'newton' newton method\n",
    "        \"\"\"\n",
    "        self.method = method\n",
    "        self.max_iter = max_iter\n",
    "        self.tol = tol\n",
    "        self.alpha = alpha\n",
    "    def _init_parameters(self, features, labels):\n",
    "        self.n = features.shape[0]\n",
    "        self.p = features.shape[1]\n",
    "        self.beta =  np.zeros((self.p+1,1))\n",
    "        self.X = np.hstack((np.ones((self.n,1)),features))\n",
    "        self.Y = labels\n",
    "        \n",
    "    def _p1(self, beta,feature):\n",
    "        \"\"\"\n",
    "        feature: (p+1) X 1 维  \n",
    "        \"\"\"\n",
    "        return(1/(1+np.exp(-(feature @ beta))))\n",
    "    \n",
    "    def _logloss(self, beta1):\n",
    "        sum1 = 0\n",
    "        for i in range(self.n):\n",
    "            sum1 += - self.Y[i] * self.X[i,:] @  beta1 + np.log(1+np.exp(self.X[i,:] @  beta1))\n",
    "        return(sum1/self.n)\n",
    "    \n",
    "    def _train_gd(self, features, labels):\n",
    "        \"\"\"\n",
    "        gradient descent method\n",
    "        \"\"\"\n",
    "        self._init_parameters(features, labels)\n",
    "        for j in range(self.max_iter):\n",
    "            beta_now = self.beta\n",
    "            loss1 = self._logloss(beta_now)\n",
    "            ### check the log and loss\n",
    "            if j % 500 ==0:\n",
    "                print('%d:%f'%(j,loss1))\n",
    "            r = []\n",
    "            for i in range(self.n):\n",
    "                r.append(self._p1(beta_now, self.X[i,:]) - self.Y[i])\n",
    "            gradient = (1/self.n) * self.X.T @ np.array(r).reshape((self.n,1))\n",
    "            beta_try = beta_now - self.alpha * gradient\n",
    "            ct_alpha = 0\n",
    "            #### make sure the loss go to downhill\n",
    "            while self._logloss(beta_try) > loss1:\n",
    "                self.alpha /= 2\n",
    "                beta_try = beta_now - self.alpha * gradient\n",
    "                ct_alpha +=1\n",
    "                if ct_alpha > 100:\n",
    "                    beta_try = beta_now\n",
    "                    break\n",
    "            self.beta = beta_try\n",
    "            #### stop condition\n",
    "            if (np.absolute(self.beta - beta_now)).mean() < self.tol:\n",
    "                break\n",
    "                \n",
    "    def _train_sgd(self, features, labels):\n",
    "        \"\"\"\n",
    "        stochastic gradient descent\n",
    "        \"\"\"\n",
    "        self._init_parameters(features, labels)\n",
    "        self.max_iter = 10000\n",
    "        self.alpha = 0.001\n",
    "        loss1 = self._logloss(self.beta)\n",
    "        for j in range(self.max_iter):\n",
    "            Ind = list(range(self.n))\n",
    "            np.random.shuffle(Ind)\n",
    "            if j % 500 ==0:\n",
    "                print('%d:%f'%(j,loss1))\n",
    "            for k in Ind:\n",
    "                beta_now = self.beta\n",
    "                loss1 = self._logloss(beta_now)\n",
    "                gradient = self.X[k,:].reshape((self.p+1,1)) * (self._p1(beta_now, self.X[k,:]) - self.Y[k])\n",
    "                beta_try = beta_now - self.alpha * gradient\n",
    "                self.beta = beta_try\n",
    "                if (np.absolute(self.beta - beta_now)).mean() < self.tol:\n",
    "                    break\n",
    "                \n",
    "    def _train_newton(self, features, labels):\n",
    "        \"\"\"\n",
    "        newton method\n",
    "        \"\"\"\n",
    "        self._init_parameters(features, labels)\n",
    "        for j in range(50):\n",
    "            loss1 = self._logloss(self.beta)\n",
    "            ### check the log and loss\n",
    "            if j % 2 ==0:\n",
    "                print('%d:%f'%(j,loss1))\n",
    "            r = []\n",
    "            w = []\n",
    "            for i in range(self.n):\n",
    "                r.append(self._p1(self.beta, self.X[i,:]) - self.Y[i])\n",
    "                w.append(np.asscalar(self._p1(self.beta, self.X[i,:]) * (1 - self._p1(self.beta, self.X[i,:]))))\n",
    "            gradient = (1/self.n) * self.X.T @ np.array(r).reshape((self.n,1))\n",
    "            hessian = (1/self.n) * self.X.T @ np.diag(w) @ self.X\n",
    "            beta_now = self.beta\n",
    "            self.beta = self.beta - np.linalg.inv(hessian) @ gradient\n",
    "            if (np.absolute(self.beta - beta_now)).mean() < 0.1 or loss1 < 0.0001:\n",
    "                break\n",
    "    \n",
    "    def train(self, features, labels):\n",
    "        if self.method == 'gd':\n",
    "            self._train_gd(features, labels)\n",
    "        if self.method == 'sgd':\n",
    "            self._train_sgd(features, labels)\n",
    "        if self.method == 'newton':\n",
    "            self._train_newton(features, labels)\n",
    "            \n",
    "    def predict(self, X_pred):\n",
    "        X_pred = np.hstack((np.ones((X_pred.shape[0],1)),X_pred))\n",
    "        sav = []\n",
    "        for i in range(X_pred.shape[0]):\n",
    "            p11 = self._p1(self.beta, X_pred[i,:])\n",
    "            if p11 >= 0.5:\n",
    "                sav.append(1.0)\n",
    "            else:\n",
    "                sav.append(0.0)\n",
    "        return(np.array(sav))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size = 0.3, random_state = 71)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 批量梯度下降法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:0.693147\n",
      "500:0.013442\n",
      "1000:0.006936\n",
      "1500:0.004715\n",
      "2000:0.003587\n",
      "2500:0.002902\n",
      "3000:0.002441\n",
      "3500:0.002109\n",
      "4000:0.001858\n",
      "4500:0.001661\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Logistic = LR()\n",
    "Logistic.train(X_train,y_train)\n",
    "Logistic.predict(X_test) == y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.46585927],\n",
       "       [-0.73805583],\n",
       "       [-2.68092378],\n",
       "       [ 4.02513816],\n",
       "       [ 1.76894571]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Logistic.beta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 牛顿迭代法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:0.693147\n",
      "2:0.050291\n",
      "4:0.007132\n",
      "6:0.001031\n",
      "8:0.000148\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Logistic_n = LR(method='newton')\n",
    "Logistic_n.train(X_train,y_train)\n",
    "Logistic_n.predict(X_test) == y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 3.81918051],\n",
       "       [-1.79434612],\n",
       "       [-4.94152943],\n",
       "       [ 5.4720225 ],\n",
       "       [ 7.25666619]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Logistic_n.beta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 随机梯度下降"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:0.693147\n",
      "500:0.023867\n",
      "1000:0.019200\n",
      "1500:0.016948\n",
      "2000:0.015531\n",
      "2500:0.014398\n",
      "3000:0.013462\n",
      "3500:0.012585\n",
      "4000:0.012018\n",
      "4500:0.011469\n",
      "5000:0.010964\n",
      "5500:0.010555\n",
      "6000:0.010232\n",
      "6500:0.009950\n",
      "7000:0.009675\n",
      "7500:0.009430\n",
      "8000:0.009210\n",
      "8500:0.009014\n",
      "9000:0.008832\n",
      "9500:0.008662\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Logistic_s = LR(method='sgd')\n",
    "Logistic_s.train(X_train,y_train)\n",
    "Logistic_s.predict(X_test) == y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.33330666],\n",
       "       [-0.52684949],\n",
       "       [-1.91391143],\n",
       "       [ 2.86784246],\n",
       "       [ 1.24277164]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Logistic_s.beta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sklearn包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\software\\ANACONDA3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LRSK = LogisticRegression()\n",
    "LRSK.fit(X_train,y_train)\n",
    "LRSK.predict(X_test) == y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.24002507])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LRSK.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.38039368, -1.35538395,  2.02958925,  0.88941473]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LRSK.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 总结\n",
    "- 从预测正确率来说，三种方法都和Skleran包一样对测试集达到了100%的预测，说明该程序还是有效的,应该没有什么重大的bug。、\n",
    "- 从loss下降角度来说，三种方法的loss都在下降，也侧面说明了程序无重大bug。另外可以明显的看出牛顿迭代的loss下降明显比其他两者快得多，另外因为这里数据量较少，没能体现出随机梯度下降和批量梯度下降的差距。\n",
    "- 另外，观察四种方法得到的超平面的参数，似乎都不太一样。首先可能是由于每种方法得出的beta的scale不一样，导致所有的看去都不太相同。另外由于该数据可能接近线性可分，所以能使其loss最低的超平面不唯一。 最后，sklearn包中有正则化过程，我写的里面没有，所以这也是原因之一。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
